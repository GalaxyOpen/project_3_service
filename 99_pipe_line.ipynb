{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DA317\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ models/yolov10n-face.pt appears to require 'dill', which is not in ultralytics requirements.\n",
      "AutoInstall will run now for 'dill' but this feature will be removed in the future.\n",
      "Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official YOLOv8 model, i.e. 'yolo predict model=yolov8n.pt'\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['dill'] not found, attempting AutoUpdate...\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Installing collected packages: dill\n",
      "Successfully installed dill-0.3.8\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 5.5s, installed 1 package: ['dill']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "0: 320x640 1 Boy, 1 Hat, 1 Human face, 1 Man, 1528.3ms\n",
      "Speed: 3.4ms preprocess, 1528.3ms inference, 10.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 256x512 1 panic, 168.6ms\n",
      "Speed: 3.0ms preprocess, 168.6ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 512)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.models import YOLOv10\n",
    "\n",
    "test_image = cv2.imread('test.png')\n",
    "\n",
    "model_object_detect = YOLO('models/yolov8x-oiv7.pt')\n",
    "model_face_emmotion = YOLOv10('models/yolov10n-face.pt')\n",
    "\n",
    "od = model_object_detect(test_image)[0]\n",
    "fc = model_face_emmotion(test_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# face 모델 라벨\n",
    "emotion_mapping = {0 : '분노', 1 : '슬픔', 2 : '공포', 3 : '기쁨'}\n",
    "\n",
    "# oiv7 모델 라벨 JSON 파일에서 딕셔너리 읽기\n",
    "with open('models/oiv7_jabels.json', 'r') as file:\n",
    "    oiv7_jabels = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_fc = [emotion_mapping[int(i)] for i in fc.boxes.cls]\n",
    "label_od = [oiv7_jabels[str(int(i))] for i in od.boxes.cls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = label_fc + label_od\n",
    "exception_lst = ['인간의 얼굴','의류','남자','여자','소년','소녀'] # 텍스트 입력 제외 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['공포', '인간의 얼굴', '모자', '남자', '소년']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_intput_text = ''\n",
    "for i in all_labels:\n",
    "    if i not in exception_lst:\n",
    "        text_intput_text +=i + ','\n",
    "\n",
    "text_intput_text = text_intput_text[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'공포,모자'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_intput_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값: 공포,모자\n",
      "출력값: 무서운 분위기가 느껴지네요! 😱 어떤 상황인지 궁금해요!\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# 저장된 모델과 토크나이저 로드\n",
    "model_save_path = 'models/t5/model/'\n",
    "tokenizer_save_path = 'models/t5/model/tokenizer/'\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_save_path)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(tokenizer_save_path)\n",
    "\n",
    "# 테스트 입력\n",
    "test_input = text_intput_text\n",
    "# 입력 토큰화\n",
    "input_ids = tokenizer.encode(test_input, return_tensors='pt')\n",
    "\n",
    "# 모델 예측\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "# 예측 결과 디코딩\n",
    "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"입력값: {test_input}\")\n",
    "print(f\"출력값: {predicted_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DA317\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1:\n",
      "입력값 : 공포,모자 \n",
      "출력값 : 무서운 분위기네요! 🪨자 자연의 모씄습이 정말 독특하네요! 😱🛏️🧣️ 분위기�\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 저장된 모델 및 토크나이저 불러오기\n",
    "model_path = 'models/gpt2/models/'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path + '/tokenizer')\n",
    "\n",
    "# 평가 모드로 변경\n",
    "model.eval()\n",
    "\n",
    "def generate_text(prompt, model, tokenizer, max_length=128, num_return_sequences=1):\n",
    "    # 입력 텍스트를 토큰화\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # 생성 인자를 설정하여 모델이 텍스트를 생성\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=30,\n",
    "        top_k=50,\n",
    "        top_p=0.85,\n",
    "        temperature=1.7,\n",
    "        do_sample=True,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # 생성된 텍스트를 디코딩\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# 예시: \"prompt\"에 원하는 문장을 넣어서 결과를 확인\n",
    "prompt = f\"입력값 : {test_input} \\n출력값 :\"\n",
    "generated_texts = generate_text(prompt, model, tokenizer)\n",
    "    \n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\")\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
