{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DA317\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ models/yolov10n-face.pt appears to require 'dill', which is not in ultralytics requirements.\n",
      "AutoInstall will run now for 'dill' but this feature will be removed in the future.\n",
      "Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official YOLOv8 model, i.e. 'yolo predict model=yolov8n.pt'\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['dill'] not found, attempting AutoUpdate...\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Installing collected packages: dill\n",
      "Successfully installed dill-0.3.8\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 5.5s, installed 1 package: ['dill']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "0: 320x640 1 Boy, 1 Hat, 1 Human face, 1 Man, 1528.3ms\n",
      "Speed: 3.4ms preprocess, 1528.3ms inference, 10.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 256x512 1 panic, 168.6ms\n",
      "Speed: 3.0ms preprocess, 168.6ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 512)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.models import YOLOv10\n",
    "\n",
    "test_image = cv2.imread('test.png')\n",
    "\n",
    "model_object_detect = YOLO('models/yolov8x-oiv7.pt')\n",
    "model_face_emmotion = YOLOv10('models/yolov10n-face.pt')\n",
    "\n",
    "od = model_object_detect(test_image)[0]\n",
    "fc = model_face_emmotion(test_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# face ëª¨ë¸ ë¼ë²¨\n",
    "emotion_mapping = {0 : 'ë¶„ë…¸', 1 : 'ìŠ¬í””', 2 : 'ê³µí¬', 3 : 'ê¸°ì¨'}\n",
    "\n",
    "# oiv7 ëª¨ë¸ ë¼ë²¨ JSON íŒŒì¼ì—ì„œ ë”•ì…”ë„ˆë¦¬ ì½ê¸°\n",
    "with open('models/oiv7_jabels.json', 'r') as file:\n",
    "    oiv7_jabels = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_fc = [emotion_mapping[int(i)] for i in fc.boxes.cls]\n",
    "label_od = [oiv7_jabels[str(int(i))] for i in od.boxes.cls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = label_fc + label_od\n",
    "exception_lst = ['ì¸ê°„ì˜ ì–¼êµ´','ì˜ë¥˜','ë‚¨ì','ì—¬ì','ì†Œë…„','ì†Œë…€'] # í…ìŠ¤íŠ¸ ì…ë ¥ ì œì™¸ ëª©ë¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ê³µí¬', 'ì¸ê°„ì˜ ì–¼êµ´', 'ëª¨ì', 'ë‚¨ì', 'ì†Œë…„']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_intput_text = ''\n",
    "for i in all_labels:\n",
    "    if i not in exception_lst:\n",
    "        text_intput_text +=i + ','\n",
    "\n",
    "text_intput_text = text_intput_text[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ê³µí¬,ëª¨ì'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_intput_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ê°’: ê³µí¬,ëª¨ì\n",
      "ì¶œë ¥ê°’: ë¬´ì„œìš´ ë¶„ìœ„ê¸°ê°€ ëŠê»´ì§€ë„¤ìš”! ğŸ˜± ì–´ë–¤ ìƒí™©ì¸ì§€ ê¶ê¸ˆí•´ìš”!\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_save_path = 'models/t5/model/'\n",
    "tokenizer_save_path = 'models/t5/model/tokenizer/'\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_save_path)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(tokenizer_save_path)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì…ë ¥\n",
    "test_input = text_intput_text\n",
    "# ì…ë ¥ í† í°í™”\n",
    "input_ids = tokenizer.encode(test_input, return_tensors='pt')\n",
    "\n",
    "# ëª¨ë¸ ì˜ˆì¸¡\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ë””ì½”ë”©\n",
    "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"ì…ë ¥ê°’: {test_input}\")\n",
    "print(f\"ì¶œë ¥ê°’: {predicted_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DA317\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1:\n",
      "ì…ë ¥ê°’ : ê³µí¬,ëª¨ì \n",
      "ì¶œë ¥ê°’ : ë¬´ì„œìš´ ë¶„ìœ„ê¸°ë„¤ìš”! ğŸª¨ì ìì—°ì˜ ëª¨ì”„ìŠµì´ ì •ë§ ë…íŠ¹í•˜ë„¤ìš”! ğŸ˜±ğŸ›ï¸ğŸ§£ï¸ ë¶„ìœ„ê¸°ï¿½\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# ì €ì¥ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model_path = 'models/gpt2/models/'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path + '/tokenizer')\n",
    "\n",
    "# í‰ê°€ ëª¨ë“œë¡œ ë³€ê²½\n",
    "model.eval()\n",
    "\n",
    "def generate_text(prompt, model, tokenizer, max_length=128, num_return_sequences=1):\n",
    "    # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # ìƒì„± ì¸ìë¥¼ ì„¤ì •í•˜ì—¬ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=30,\n",
    "        top_k=50,\n",
    "        top_p=0.85,\n",
    "        temperature=1.7,\n",
    "        do_sample=True,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ë””ì½”ë”©\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# ì˜ˆì‹œ: \"prompt\"ì— ì›í•˜ëŠ” ë¬¸ì¥ì„ ë„£ì–´ì„œ ê²°ê³¼ë¥¼ í™•ì¸\n",
    "prompt = f\"ì…ë ¥ê°’ : {test_input} \\nì¶œë ¥ê°’ :\"\n",
    "generated_texts = generate_text(prompt, model, tokenizer)\n",
    "    \n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\")\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
